# ============================================
# Elastic RAG Configuration Template
# ============================================
# Copy this file to .env and fill in values
# NEVER commit .env to version control!
#
# Environment variables use double underscore (__) as a delimiter for nested config.
# For example: LMSTUDIO__BASE_URL maps to the lmstudio.base_url setting
# Variable names are case-insensitive.

# ============================================
# Application Settings
# ============================================
# General application configuration for the FastAPI server

# Server host address (0.0.0.0 binds to all interfaces)
APP__HOST=0.0.0.0

# Server port number
APP__PORT=8000

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
APP__LOG_LEVEL=INFO

# Application environment: development, staging, production
APP__ENVIRONMENT=development


# ============================================
# Generic Embedder Configuration (Recommended)
# ============================================
# Provider-agnostic configuration that works with any OpenAI-compatible API
# This allows easy switching between LMStudio, OpenAI, Anthropic, etc.
# If set, these values take precedence over LMSTUDIO__ settings for embeddings

# Provider name (for logging/debugging)
# Examples: lmstudio, openai, anthropic, azure
EMBEDDER__PROVIDER=lmstudio

# Provider API base URL
# LMStudio: http://localhost:1234/v1
# OpenAI: https://api.openai.com/v1
EMBEDDER__BASE_URL=http://localhost:1234/v1

# Embedding model name
# LMStudio: openai/text-embedding-bge-m3, nomic-embed-text
# OpenAI: text-embedding-3-small, text-embedding-ada-002
EMBEDDER__MODEL=openai/text-embedding-bge-m3

# API key for the provider
# LMStudio: use "lmstudio" or any dummy value
# OpenAI: your actual API key (keep secret!)
EMBEDDER__API_KEY=lmstudio

# Request timeout in seconds
EMBEDDER__TIMEOUT=30


# ============================================
# Generic LLM Configuration (Recommended)
# ============================================
# Provider-agnostic configuration for chat completion models
# This allows easy switching between LMStudio, OpenAI, Anthropic, etc.
# If set, these values take precedence over LMSTUDIO__ settings for chat completions

# Provider name (for logging/debugging)
# Examples: lmstudio, openai, anthropic, azure
LLM__PROVIDER=lmstudio

# Provider API base URL
# LMStudio: http://localhost:1234/v1
# OpenAI: https://api.openai.com/v1
# Anthropic: https://api.anthropic.com/v1
LLM__BASE_URL=http://localhost:1234/v1

# Chat completion model name
# LMStudio: openai/qwen3-30b-a3b-mlx, llama-3.2-3b-instruct
# OpenAI: gpt-4, gpt-3.5-turbo, gpt-4-turbo
# Anthropic: claude-3-opus-20240229, claude-3-sonnet-20240229
LLM__MODEL=openai/qwen3-30b-a3b-mlx

# API key for the provider
# LMStudio: use "lmstudio" or any dummy value
# OpenAI/Anthropic: your actual API key (keep secret!)
LLM__API_KEY=lmstudio

# Request timeout in seconds
LLM__TIMEOUT=30

# Default sampling temperature (0.0 = deterministic, 2.0 = very creative)
# For factual Q&A, use lower values (0.3-0.7). For creative tasks, use higher values (0.8-1.5)
LLM__TEMPERATURE=0.3

# Default maximum tokens in response (model-dependent, check your model's limits)
# Common values: 1000-4000 for short answers, 8000-15000 for detailed responses
# Note: qwen3-30b supports up to 40,000 tokens
LLM__MAX_TOKENS=15000


# ============================================
# LMStudio Configuration (Local LLM)
# ============================================
# LMStudio-specific settings for backward compatibility
# If LLM__ settings above are not provided, these will be used for chat completions
# If EMBEDDER__ settings above are not provided, these will be used for embeddings
# Make sure LMStudio is running and models are loaded before starting the app

# LMStudio API base URL (must include /v1)
# Default: http://localhost:1234/v1
LMSTUDIO__BASE_URL=http://localhost:1234/v1

# Embedding model name (must be loaded in LMStudio)
# Recommended: nomic-embed-text, all-minilm-l6-v2, text-embedding-bge-m3
LMSTUDIO__EMBEDDING_MODEL=openai/text-embedding-bge-m3

# Chat completion model name (must be loaded in LMStudio)
# Examples: llama-3.2-3b-instruct, mistral-7b-instruct-v0.2
LMSTUDIO__CHAT_MODEL=openai/qwen3-30b-a3b-mlx

# Optional API key (usually not needed for local LMStudio)
# Leave empty or use a dummy value like "lmstudio"
LMSTUDIO__API_KEY=lmstudio

# Request timeout in seconds
LMSTUDIO__TIMEOUT=30


# ============================================
# Elasticsearch Configuration
# ============================================
# Elasticsearch is used for document storage and vector similarity search

# Elasticsearch hostname
# Use "elasticsearch" for Docker Compose, "localhost" for local ES
ELASTICSEARCH__HOST=elasticsearch

# Elasticsearch port
ELASTICSEARCH__PORT=9200

# Index name for storing documents
ELASTICSEARCH__INDEX=documents

# Optional username for authentication (leave empty if auth disabled)
ELASTICSEARCH__USERNAME=

# Optional password for authentication (leave empty if auth disabled)
ELASTICSEARCH__PASSWORD=


# ============================================
# Chunking Configuration
# ============================================
# Configuration for splitting documents into smaller chunks

# Chunk size in tokens/characters (must be between 100 and 2000)
# Recommended: 512 for most use cases
CHUNKING__SIZE=1024

# Overlap between chunks in tokens (maintains context across boundaries)
# Recommended: 10% of chunk size (e.g., 50 for size 512)
CHUNKING__OVERLAP=256


# ============================================
# Retrieval Configuration
# ============================================
# Configuration for retrieving relevant documents during RAG queries

# Number of top documents to retrieve (top-k)
# Higher values provide more context but may introduce noise
RETRIEVAL__TOP_K=5

# Minimum similarity score threshold (0.0 to 1.0)
# Documents below this threshold are filtered out
# Recommended: 0.7 for good balance of precision and recall
RETRIEVAL__SIMILARITY_THRESHOLD=0.7


# ============================================
# Circuit Breaker Configuration
# ============================================
# Circuit breaker prevents cascading failures by temporarily stopping
# requests to failing services

# Number of consecutive failures before opening the circuit
CIRCUIT_BREAKER__FAILURE_THRESHOLD=5

# Time in seconds to wait before attempting recovery (closed -> half-open)
CIRCUIT_BREAKER__TIMEOUT_SECONDS=60

# Maximum number of calls allowed in half-open state for testing recovery
CIRCUIT_BREAKER__HALF_OPEN_MAX_CALLS=3


# ============================================
# Health Check Configuration
# ============================================
# Configuration for health probes and monitoring

# Timeout for individual health checks in seconds
HEALTH__CHECK_TIMEOUT=1

# Maximum time to wait for application startup in seconds
HEALTH__STARTUP_TIMEOUT=30

# Interval between readiness checks in seconds
HEALTH__READINESS_INTERVAL=5


# ============================================
# File Upload Configuration
# ============================================
# Configuration for API document upload validation

# Allowed file extensions (comma-separated list)
# Supported formats: .pdf, .docx, .pptx, .html, .txt
# Example: FILE_UPLOAD__ALLOWED_EXTENSIONS=.pdf,.docx,.txt
# Note: Pydantic will parse this as a set automatically
FILE_UPLOAD__ALLOWED_EXTENSIONS=.pdf,.docx,.pptx,.html,.txt

# Maximum file size in megabytes (must be positive, max 1000 MB)
# Default: 50 MB
FILE_UPLOAD__MAX_FILE_SIZE_MB=50


# ============================================
# Gradio UI Configuration
# ============================================
# Configuration for the optional Gradio web interface

# UI server host (0.0.0.0 binds to all interfaces)
UI__HOST=0.0.0.0

# UI server port
UI__PORT=7860

# Enable Gradio public URL sharing (disabled by default for security)
# When enabled, creates a public URL accessible from anywhere
UI__SHARE=false

# FastAPI backend URL that the UI connects to
# Use http://localhost:8000 for local development
# Update for production deployments
UI__API_URL=http://localhost:8000
